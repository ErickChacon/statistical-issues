\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Statistical Issues},
            pdfauthor={Erick A. Chac칩n Montalv치n},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Statistical Issues}
\author{Erick A. Chac칩n Montalv치n}
\date{2020-08-10}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This is a work in progress of collection of topics I have been working on.

\hypertarget{how-to-read}{%
\section*{How to read}\label{how-to-read}}
\addcontentsline{toc}{section}{How to read}

\hypertarget{structure}{%
\section*{Structure}\label{structure}}
\addcontentsline{toc}{section}{Structure}

\hypertarget{software}{%
\section*{Software}\label{software}}
\addcontentsline{toc}{section}{Software}

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgments}

\hypertarget{part-introduction}{%
\part{Introduction}\label{part-introduction}}

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

In order to apply statistics and motivate advances on this area, we need to
understand other areas as well.

\hypertarget{part-statistics}{%
\part{Statistics}\label{part-statistics}}

\hypertarget{statistical-concepts}{%
\chapter{Statistical Concepts}\label{statistical-concepts}}

\begin{itemize}
\tightlist
\item
  Probability
\item
  Likelihood
\item
  Confidence Intervals
\item
  Statistical Methods
\item
  Classic Statistics
\item
  Bayesian statistics
\item
  Comparing classic and Bayesian statistics
\item
  Causality, Confounding
\end{itemize}

\hypertarget{part-inference}{%
\part{Inference}\label{part-inference}}

\hypertarget{statistical-inference}{%
\chapter{Statistical Inference}\label{statistical-inference}}

\begin{itemize}
\tightlist
\item
  Introduction to Statistical Inference
\item
  Bayesian Statistical Inference
\item
  Maximum Likelihood Inference

  \begin{itemize}
  \tightlist
  \item
    Likelihood Function

    \begin{itemize}
    \tightlist
    \item
      Maximum Likelihood Inference
    \end{itemize}
  \item
    The entropy

    \begin{itemize}
    \tightlist
    \item
      Inference with Variational Bayes
    \end{itemize}
  \item
    Intro inside inference basics
  \end{itemize}
\item
  Inference used in Machine Learning

  \begin{itemize}
  \tightlist
  \item
    More About Variational Bayes
  \end{itemize}
\end{itemize}

\hypertarget{part-modelling}{%
\part{Modelling}\label{part-modelling}}

\hypertarget{introduction-mod_intro}{%
\chapter{Introduction \{mod\_intro\}}\label{introduction-mod_intro}}

Some \emph{significant} applications are demonstrated in this chapter.

\hypertarget{example-one}{%
\section{Example one}\label{example-one}}

\hypertarget{example-two}{%
\section{Example two}\label{example-two}}

\hypertarget{part-programming}{%
\part{Programming}\label{part-programming}}

\hypertarget{final-words}{%
\chapter{Final Words}\label{final-words}}

We have finished a nice book.

\hypertarget{part-mathematics}{%
\part{Mathematics}\label{part-mathematics}}

\hypertarget{linear-algebra}{%
\chapter{Linear Algebra}\label{linear-algebra}}

\hypertarget{linear-algebra-concepts}{%
\section{Linear Algebra Concepts}\label{linear-algebra-concepts}}

\newcommand{\rv}[1]{#1}
\newcommand{\rve}[1]{\mathbf{#1}}
\newcommand{\gp}{w}
\newcommand{\gpt}{\text{GP}}
\newcommand{\mgp}{\mathbf{w}}
\newcommand{\mgpt}{\text{MGP}}

\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\vet}[1]{\mathbf{#1}}
\newcommand{\m}[1]{\mathbf{#1}}

\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\tr}{\intercal}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normsq}[1]{\left\lVert#1\right\rVert^2}
\newcommand{\diag}[1]{\text{diag}(#1)}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\cross}[2][]{#2^\tr\ifthenelse{\isempty{#1}}{#2}{{#1}}}
\newcommand{\tcross}[1]{#1#1^\tr}
\newcommand{\mve}[1]{\text{vec}({#1})}

\newcommand{\ind}[1]{\mathds{1}_{\left(#1\right)}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\real}{\mathbb{R}}

\newcommand{\df}[1]{\mathcal{#1}}

\newcommand{\logit}[1]{\text{logit}\left(#1\right)}

\newcommand{\key}[1]{\textbf{#1}}
\newcommand{\page}[1]{page #1}
\newcommand{\mypar}[1]{\noindent \textbf{#1}}
\newcommand{\degrees}{^{\circ}}
\newcommand{\eq}{\text{Equation }}
\newcommand{\se}{\text{Section }}

\emph{Linear transformations} and \emph{change of basis} are widely used in statistics, for this
reason I briefly describe the definition of these concepts and how they are related.

\hypertarget{linear-transformation}{%
\subsection{Linear Transformation}\label{linear-transformation}}

Letting \(V\) and \(W\) be vector spaces, a function \(f: V \rightarrow W\) is a linear
transformation if the additivity and scalar multiplication properties are hold for
any two vectors \(\mathbf{u}, \mathbf{v} \in V\) and a constant \(c\):
\[f(\mathbf{u}+\mathbf{v}) = f(\mathbf{u}) + f(\mathbf{v})\]
\[f(c\mathbf{v}) = cf(\mathbf{v}).\]

This concept is more common to use when working with matrices. Considering the
vector spaces \(V \in \mathbb{R}^n\) and \(W \in \mathbb{R}^m\), a matrix \(\mathbf{A}_{m \times n}\) and
the vector \(\mathbf{x} \in V\); then the function
\[f(\mathbf{x}) = \mathbf{A}\mathbf{x}\]
is a linear
transformation \(V \in \mathbb{R}^n\) to \(W \in \mathbb{R}^m\) because it holds the properties
mentioned above. In this definition, although not mentioned, we are assuming that both
\(V\) and \(W\) are defined using the \emph{standard basis} for \(\mathbb{R}^n\) and \(\mathbb{R}^m\)
respectively.

\hypertarget{change-of-basis}{%
\subsection{Change of Basis}\label{change-of-basis}}

Consider a vector \(\mathbf{u} \in \mathbb{R}^n\), it is implicitly defined using the
\emph{standard basis} \(\{\mathbf{e}_1,\dots,\mathbf{e}_n\}\) for \(\mathbb{R}^n\), such as
\(\mathbf{u}=\sum_{i=1}^n u_i \mathbf{e}_i\). In a similar manner, this vector \(\mathbf{u}\) can
also be represented in vector spaces with different \emph{basis}, this is called
\emph{change of basis}. For example, consider
the vector space \(V \in \mathbb{R}^n\) with \emph{basis} \(\{\mathbf{v}_1,\dots,\mathbf{v}_n\}\). Then, in
order to make the change of basis, it is required to find
\(\mathbf{u}_v=(u_{v_1},\dots,u_{v_n})^\intercal\) such as
\[\mathbf{u} = \sum_{i=1}^n u_{v_i} \mathbf{v}_i = \mathbf{V}\mathbf{u}_v,\]
where the \(n\times n\) matrix \(\mathbf{V}=(\mathbf{v}_1,\dots,\mathbf{v}_n)\), hence the change from
the \emph{standard basis} to the vector space \(V\) is
\[\mathbf{u}_v = \mathbf{V}^{-1}\mathbf{u},\] while the change from the vector space \(V\) to the
\emph{standard basis} is
\[\mathbf{u} = \mathbf{V}\mathbf{u}_v.\]

Now, consider another vector space \(W \in \mathbb{R}^n\) with \emph{basis}
\(\{\mathbf{w}_1,\dots,\mathbf{w}_n\}\), the vector \(\mathbf{u}_v\) defined on the space \(V\) can
also be defined on the space \(W\) as
\[\mathbf{u}_w = \mathbf{W}^{-1}\mathbf{V}\mathbf{u}_v\]
where the \(n\times n\) matrix \(\mathbf{W}=(\mathbf{w}_1,\dots,\mathbf{w}_n)\);
similarly, the vector \(\mathbf{u}_w \in W\) can be defined on the space \(V\) as
\[\mathbf{u}_v = \mathbf{V}^{-1}\mathbf{W}\mathbf{u}_w.\]
It can be seen that in both cases, the
original vector is first transformed to the space vector with \emph{standard basis}
(left-multiplying the basis matrix) and
then transformed to the desired vector space (left-multiplying the basis matrix
inverse ).

\hypertarget{change-of-basis-for-linear-transformations}{%
\subsection{Change of Basis for Linear Transformations}\label{change-of-basis-for-linear-transformations}}

Previously, we have presented a linear transformation
\(f(\mathbf{x})=\mathbf{A}\mathbf{x}:\mathbb{R}^n\rightarrow\mathbb{R}^m\)
using \emph{standard basis}. This transformation can also be represented from a vector space
\(V\) with basis \(\{\mathbf{v}_1,\dots,\mathbf{v}_n\}\) to a vector space \(W\) with basis
\(\{\mathbf{w}_1,\dots,\mathbf{w}_n\}\), then \(f': V \rightarrow W\) is defined as
\[f'(\mathbf{x}_v) = \mathbf{W}^{-1}\mathbf{A}\mathbf{V}\mathbf{x}_v,\]
where the matrices \(\mathbf{W}\) and \(\mathbf{V}\) are the basis matrix of the vector spaces \(W\)
and \(V\) respectively. The matrix multiplication \(\mathbf{W}^{-1}\mathbf{A}\mathbf{V}\) implies a
change of basis from \mathbf{V} to standard basis, the linear transformation using the
standard basis, and the change from the standard basis to the space \(W\). In cases
that \(V=W\), then the linear transformation is defined as
\[f'(\mathbf{x}_v) = \mathbf{V}^{-1}\mathbf{A}\mathbf{V}\mathbf{x}_v.\]

\hypertarget{eigenvalues-and-eigenvectors}{%
\subsection{Eigenvalues and Eigenvectors}\label{eigenvalues-and-eigenvectors}}

\emph{Eigenvalues} and \emph{eigenvectors} are used in several concepts of statistical inference
and modelling. It can be useful for dimension reduction, decomposition of
variance-covariance matrices, so on. For this reason, we provide basic details about
eigenvectors and eigenvalues and their close relationship with linear
transformations.

\hypertarget{definition}{%
\subsubsection{Definition}\label{definition}}

The eigenvector of a linear transformation \(\mathbf{A}_{n\times n}\) is a non-zero vector
\(\mathbf{v}\)
such as the linear transformation of this vector is proportional to itself:
\[\mathbf{A}\mathbf{v} = \lambda \mathbf{v} \iff (\mathbf{A}-\lambda\mathbf{I})\mathbf{v} = \mathbf{0},\]
where \(\lambda\) is the eigenvalue associated to the eigenvector \(\mathbf{v}\). The
equation above has non-zero solution if and only if
\[\det(\mathbf{A}-\lambda\mathbf{I}) = 0.\]
Then, all the eigenvalues \(\lambda\) of \(\mathbf{A}\) hold the condition above.

There is an equivalence between the linear transformation \(f(\mathbf{x}) = \mathbf{A}\mathbf{x}\),
and the eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\) and eigenvectors
\(\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\) of itself. This relationship provide more
useful
interpretation of the eigenvalues and eigenvectors, we will use the
\href{../../mathematics/linear-algebra/10-linear-transformation}{change of basis}
concept to describe it.

\hypertarget{eigendecomposition-and-geometric-interpretation}{%
\subsubsection{Eigendecomposition and geometric interpretation}\label{eigendecomposition-and-geometric-interpretation}}

Considering a vector space \(V\) with basis \(\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}\),
any vector \(\mathbf{x} \in \mathbb{R}^n\) can be represented as \(\mathbf{V}\mathbf{x}_v\),
where \(\mathbf{x}_v\) is the representation of \(\mathbf{x}\) using the matrix of basis
\(\mathbf{V}=(\mathbf{v}_1, \dots, \mathbf{v}_n)\) of the vector space \(V\). Then, the linear
transformation can be expressed as
\[f(\mathbf{x}) = \mathbf{A}\mathbf{x} = \mathbf{A}\mathbf{V}\mathbf{x}_v = \mathbf{V}\mathbf{D}\mathbf{x}_v,\]
where the diagonal matrix \(D=\text{diag}(\lambda_1, \dots, \lambda_n)\) and the last
equivalence hold because \(\mathbf{A}\mathbf{v}_i=\mathbf{v}_i\lambda_i\). Finally, expressing
\(\mathbf{x}_v\) in terms of the vector \(\mathbf{x}\) defined on the standard basis, we
obtain that
\[f(\mathbf{x}) = \mathbf{V}\mathbf{D}\mathbf{V}^{-1}\mathbf{x},\]
the equality \(\mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{V}^{-1}\) is called \emph{eigendecomposition}.
Hence, the linear transformation is equivalent to the following:
change the basis of \(\mathbf{x}\) to the vector space \(V\)
, apply the diagonal linear transformation \(D\) and return to the space with
standard basis. Geometrically, you can think of
\(\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}\) as the basis of vectorial space \(V\) where
the transformation \(\mathbf{A}\) becomes only an scaling transformation \(\mathbf{D}\) and the
eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\) are the scaling factor in
direction of the corresponding eigenvector \(\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\).

\hypertarget{basis-properties}{%
\subsubsection{Basis properties}\label{basis-properties}}

There are certain properties the are useful for statistical modelling such as:

\begin{itemize}
\tightlist
\item
  Trace of \(\mathbf{A}\) is equals to the sum of the eigenvalues.
\item
  Determinant of \(\mathbf{A}\) is equals to the sum of the eigenvalues.
\item
  If \(\mathbf{A}\) is symmetric, then all eigenvalues are real.
\item
  If \(\mathbf{A}\) is positive definite, then all eigenvalues are positive.
\end{itemize}

Note that, some of these properties can be explained using the \emph{eigendecomposition}
\(\mathbf{A} = \mathbf{V}\mathbf{D}\mathbf{V}^{-1}\).

\hypertarget{here-we-go}{%
\chapter{Here we go}\label{here-we-go}}

\bibliography{book.bib,packages.bib}

\end{document}
