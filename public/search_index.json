[
["index.html", "Statistical Issues Preface How to read Structure Software Acknowledgments", " Statistical Issues Erick A. Chacón Montalván 2021-03-01 Preface This is a work in progress of collection of topics I have been working on. The main focus will be on putting together concepts of Statistics and Probability, inference, modelling, programming, mathematics and optimization. How to read Structure Software Acknowledgments "],
["110-intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction In order to apply statistics and motivate advances on this area, we need to understand other areas as well. This is a test. "],
["210-statistics.html", "Chapter 2 Statistical Concepts", " Chapter 2 Statistical Concepts Probability Likelihood Confidence Intervals Statistical Methods Classic Statistics Bayesian statistics Comparing classic and Bayesian statistics Causality, Confounding "],
["310-measure-theory.html", "2.1 Measure Theory", " 2.1 Measure Theory This deals with the issue of assigning a measure (e.g. length, area volume) to certain sets. Properties we should expect form a measure \\(\\mu\\) are: 2.1.1 Properties Well-definedness: it should take values in \\([0, \\infty]\\) and \\(\\mu(\\emptyset) = 0\\). Additiveness: if \\(A \\cup B = \\emptyset\\) then \\(\\mu(A \\cup B) = \\mu(A) + \\mu(B)\\). Invariant (addtional): under congruences and translations as Lebesgue measure on \\(R^n\\). Imagine that using these properties we try to obtain the area of a circle. We will notice that finite additivity is not enough, we also need to introduce \\(sigma\\)-additivity on additiveness property. 2.1.2 Set algebra and countability \\[\\begin{align} A \\cup B &amp;= \\{x: x \\in A ~\\text{or}~ x \\in B ~\\text{or}~ x \\in A ~\\text{and}~ B\\}\\\\ A \\cap B &amp;= \\{x: x \\in A ~\\text{and}~ B\\}\\\\ A \\setminus B &amp;= \\{x: x \\in A ~\\text{and}~ B\\} \\end{align}\\] \\(A \\dot{\\cup} B\\) represents disjoint union, \\(A \\subset B\\) means A is contained in B including \\(A = B\\). \\(A^c := X \\setminus A\\) for \\(A \\subset B\\) is the complement of A relative to \\(X\\). Distributive laws: \\[\\begin{align} A \\cap (B \\cup C) &amp; = (A \\cap B) \\cup (A \\cap C) \\\\ A \\cap (B \\cup C) &amp; = (A \\cap B) \\cup (A \\cap C) \\end{align}\\] Morgan’s identities \\[\\begin{align} \\left(\\bigcap_{i \\in I} A_i\\right)^c &amp;= \\bigcup_{i \\in I} A_i^c \\\\ \\left(\\bigcup_{i \\in I} A_i\\right)^c &amp;= \\bigcap_{i \\in I} A_i^c \\end{align}\\] A function \\(f: X \\rightarrow Y\\) is: injective (one-to-one) \\(\\Leftrightarrow\\) \\(f(x) = f(x&#39;)\\) implies \\(x = x&#39;\\), surjective (onto) \\(\\Leftrightarrow\\) \\(f(X) := {f(x) \\in Y: x \\in X} = Y\\), bijective \\(\\Leftrightarrow\\) \\(f(.)\\) is injective and surjective. Set operations shown before and direct images under a funtion \\(f\\) are not necessarily compatible: \\[\\begin{align} f(A \\cup B) &amp;= f(A) \\cup f(B) \\\\ f(A \\cap B) &amp;\\neq f(A) \\cap f(B) \\\\ f(A \\setminus B) &amp;\\neq f(A) \\setminus f(B) \\\\ \\end{align}\\] While inverse images and set operations are always compatible. The inverse mapping \\(f_{-1}\\) maps subsets \\(C \\subset Y\\) into subsets of \\(X\\): \\[\\begin{equation} f^{-1}(C) := \\{x \\in X: f(x) \\in C\\} \\in X, ~\\text{for all}~ C \\subset Y. \\end{equation}\\] It follows that: \\[\\begin{align} f^{-1}(\\bigcup_{i \\in I} C_i) &amp;= \\bigcup_{i \\in I} f^{-1}(C_i), \\\\ f^{-1}(\\bigcap_{i \\in I} C_i) &amp;= \\bigcap_{i \\in I} f^{-1}(C_i), \\\\ f^{-1}(C \\setminus D) &amp;= f^{-1}(C) \\setminus f^{-1}(D) \\end{align}\\] 2.1.3 \\(\\sigma\\)-Algebras A measure function must be defined on a system of sets stable under repetition of set operations (\\(\\cup, \\cap, ^c\\)) countably many times. A \\(\\sigma\\)-algebra \\(\\mathcal{A}\\) on a set \\(X\\) is a family of subsets of \\(X\\) with the following properties: \\[\\begin{align} X &amp;\\in \\mathcal{A}, \\\\ A \\in \\mathcal{A} &amp; \\Rightarrow A^c \\in \\mathcal{A}, \\\\ (A_n)_{n\\in N} \\subset \\mathcal{A} &amp; \\Rightarrow \\bigcup_{n \\in N} A_n \\in \\mathcal{A}. \\end{align}\\] Based on these definitions, we can obtain some properties: \\(\\emptyset \\in \\mathcal{A}\\) \\(A, B \\in \\mathcal{A} \\Rightarrow A \\cup B \\in \\mathcal{A}\\) \\((A_n)_{n \\in N} \\subset \\mathcal{A} \\Rightarrow \\cap_{n \\in N} A_n \\in \\mathcal{A}\\) "],
["310-statistical-inference.html", "Chapter 3 Statistical Inference 3.1 Classical Inference Techniques 3.2 Bayesian Inference Techniques", " Chapter 3 Statistical Inference It is a nice experience. Introduction to Statistical Inference Bayesian Statistical Inference Maximum Likelihood Inference Likelihood Function Maximum Likelihood Inference The entropy Inference with Variational Bayes Intro inside inference basics Inference used in Machine Learning More About Variational Bayes 3.1 Classical Inference Techniques Maximum likelihood estimation Expectation maximization Iteratively reweighted least squares Restricted maximum likelihood estimation 3.2 Bayesian Inference Techniques Rejection sampling Importance sampling Markov chain Monte Carlo Gibbs sampling Metropolis Hasting Laplace Approximation Integrated nested Laplace approximation Variational Bayes "],
["410-modelling.html", "Chapter 4 Introduction {mod_intro} 4.1 Example one 4.2 Generalized additive models 4.3 GAM 4.4 GAM 4.5 Practice MGCV", " Chapter 4 Introduction {mod_intro} Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Generalized additive models splines A cubic spline is a piecewise polynomial with the property of being continuously differentiable until second order. The number of knots act as an smoothing parameter in unpenalized splines. Knots defined based on the quantiles of the data make the spline flexible in dense areas and less flexible in sparse areas, which is desirable. In general, it is more appropiate to select more knots than expected and use a penalty term to control for smothness avoiding the need to select number of knots. For \\(l\\) knots and degree \\(r\\), the space of polynomial splines is a vector space with dimension equals to the number of free parameters. \\(l-1\\) polynomial functions of degree \\(r\\) have \\((r+1)(l-1)\\) parameters. The condition of \\(r-1\\) times continuously differentiable generate \\(r\\) constrains for all the \\(l-2\\) inside knots. Then, the number of free parameters is \\((r+1)(l-1) - r(l-2) = r+l-1\\). Natural splines assumes that the curvature, the second derivative, at the first and last knot is zero. Then, a natural cubic spline will have \\(l\\) free parameters. Simon Wood 2016 B-splines, whose construction from polynomial pieces gives them many attractive computational properties, as described by de Boor (1978). 2016 donnell 4.3 GAM smoothing bases natural cubic splines are smoothest interpolators cubic smoothing splines cubic regression splines cyclic cubic regression spline p-splines thin plate regression splines tensor products smooths polynomial spline cubic spline on each data regression spline on knots evenly spaced or with quantiles penalizing to avoid overfitting for \\(\\lambda\\) known, it is still a augmented linear model select \\(\\lambda\\) with ordinary cross validation computational and invariante advante og generalized cross validation Implementation initialize lambda given lambda, obtain beta compute gcv, and interate with previous step Identifiability, constrain one the rest of intercept parameters to zero. 4.4 GAM penalized likelihood maximization no simple trick to produce an unpenalized glm whose likelihood is equivalent to the penalized likelihood of the GAM penalized iteratively re-weighted least squares The suggestion of representing GAMs using spline like penalized regression smoothers was made in section 9.3.6 of Hastie and Tibshirani (1990) 4.5 Practice MGCV 4.5.1 Basics of gam model When the relationship is almost linear \\(df = 1\\), the confidence interval are zero when the estimates is zero due to the identifiability constrain. This restriction sets the mean value of \\(f\\) to zero, such as there is no uncertainty when \\(f = 0\\). The points on the smoothed effects are just the partial residual, which simple are the Pearson residuals plus the smooth function for the corresponding covariate being plotted. Considering an initial model with \\(k_1\\) knots and \\(df&lt;k_1 -1\\); then, increasing the number of knot to \\(k_2\\), can modified the number of effective degrees of freedom. It happens because different subspace of functions are obtained when \\(k=k_1\\) or \\(k+k_2\\) for a particular \\(df\\). Smoother functions can be obtained introducing and additional parameter to the GCV score \\(\\gamma\\). For example, \\(\\gamma = 1.4\\) is suggested to avoid overfitting without compromising model fit. 4.5.2 Smoothing several variables We can use thin plate regression spline or tensor products. "],
["450-survival-analysis.html", "4.6 Survival Analysis 4.7 sub1 4.8 sub2", " 4.6 Survival Analysis Thi sis a test. Great. The probability of survival until time \\(t\\) or the probability that the event will occurr after time \\(t\\) is defined as: \\[\\begin{equation} S(t) = P(T &gt; t) = 1 - F(t) \\end{equation}\\] The hazard (instantaneous risk) \\[\\begin{equation} h(t) = P(t &lt; T \\leq t + t\\delta \\mid T &gt; t) = \\frac{f(t)}{S(t)} \\end{equation}\\] Notice that it is enough to know one of the terms \\(f(t), S(t), h(t)\\) to derive the remaining terms: \\(f(t)\\): \\(S(t) = \\int_{t}^{\\infty}f(x)dx\\); \\(h(t) = \\frac{f(t)}{\\int_{t}^{\\infty}f(x)dx}\\) \\(S(t)\\): \\(f(t) = \\frac{dS(t)}{dt}\\); \\(h(t) = \\frac{S&#39;(t)}{S(t)} = -\\frac{dlog(S(t))}{dt}\\) \\(h(t)\\): \\(S(t) = exp\\left(\\int_0^t h(u)du\\right)\\); \\(f(t) = \\frac{ d\\left(1-exp\\left(\\int_0^t h(u)du\\right)\\right)}{dt}\\) This type of analysis assumes a hazard function that defines the probability of the occurrence of an event. En general it is called survival because of the interest on. I should not neet to update. This is strange because I can not see the file. library(ggplot2) 4.7 sub1 4.8 sub2 Let’s do a plot. This is a simpl plot for subsection 2. I do not understand what happen, but it is working well. Entr is amazing, so great! n &lt;- 1000 x &lt;- rnorm(n) y &lt;- rpois(3, 10) plot(1:10, col = 2, pch = 19) # ggplot(iris) + # geom_histogram(aes(Sepal.Length)) What is the problem then? y = 1:10 # hist(rnorm(10000), 20) Nice. Let’s start with book. What is the issue? Again the same problem. If this is a new chapter it works fine. I can see that. However there is a problem for a new file inside a chapter. Survival analysis is the study of the time until the occurrence of an event. There is something weird here because this is not getting updated. Now it looks fine. What was the problem? Now? "],
["510-programming.html", "Chapter 5 Final Words", " Chapter 5 Final Words We have finished a nice book. "],
["610-mathematics.html", "Chapter 6 Linear Algebra 6.1 Linear Algebra Concepts", " Chapter 6 Linear Algebra 6.1 Linear Algebra Concepts Linear transformations and change of basis are widely used in statistics, for this reason I briefly describe the definition of these concepts and how they are related. 6.1.1 Linear Transformation Letting \\(V\\) and \\(W\\) be vector spaces, a function \\(f: V \\rightarrow W\\) is a linear transformation if the additivity and scalar multiplication properties are hold for any two vectors \\(\\ve{u}, \\ve{v} \\in V\\) and a constant \\(c\\): \\[f(\\ve{u}+\\ve{v}) = f(\\ve{u}) + f(\\ve{v})\\] \\[f(c\\ve{v}) = cf(\\ve{v}).\\] This concept is more common to use when working with matrices. Considering the vector spaces \\(V \\in \\real^n\\) and \\(W \\in \\real^m\\), a matrix \\(\\m{A}_{m \\times n}\\) and the vector \\(\\ve{x} \\in V\\); then the function \\[f(\\ve{x}) = \\m{A}\\ve{x}\\] is a linear transformation \\(V \\in \\real^n\\) to \\(W \\in \\real^m\\) because it holds the properties mentioned above. In this definition, although not mentioned, we are assuming that both \\(V\\) and \\(W\\) are defined using the standard basis for \\(\\real^n\\) and \\(\\real^m\\) respectively. 6.1.2 Change of Basis Consider a vector \\(\\ve{u} \\in \\real^n\\), it is implicitly defined using the standard basis \\(\\{\\ve{e}_1,\\dots,\\ve{e}_n\\}\\) for \\(\\real^n\\), such as \\(\\ve{u}=\\sum_{i=1}^n u_i \\ve{e}_i\\). In a similar manner, this vector \\(\\ve{u}\\) can also be represented in vector spaces with different basis, this is called change of basis. For example, consider the vector space \\(V \\in \\real^n\\) with basis \\(\\{\\ve{v}_1,\\dots,\\ve{v}_n\\}\\). Then, in order to make the change of basis, it is required to find \\(\\ve{u}_v=(u_{v_1},\\dots,u_{v_n})^\\tr\\) such as \\[\\ve{u} = \\sum_{i=1}^n u_{v_i} \\ve{v}_i = \\m{V}\\ve{u}_v,\\] where the \\(n\\times n\\) matrix \\(\\m{V}=(\\ve{v}_1,\\dots,\\ve{v}_n)\\), hence the change from the standard basis to the vector space \\(V\\) is \\[\\ve{u}_v = \\m{V}^{-1}\\ve{u},\\] while the change from the vector space \\(V\\) to the standard basis is \\[\\ve{u} = \\m{V}\\ve{u}_v.\\] Now, consider another vector space \\(W \\in \\real^n\\) with basis \\(\\{\\ve{w}_1,\\dots,\\ve{w}_n\\}\\), the vector \\(\\ve{u}_v\\) defined on the space \\(V\\) can also be defined on the space \\(W\\) as \\[\\ve{u}_w = \\m{W}^{-1}\\m{V}\\ve{u}_v\\] where the \\(n\\times n\\) matrix \\(\\m{W}=(\\ve{w}_1,\\dots,\\ve{w}_n)\\); similarly, the vector \\(\\ve{u}_w \\in W\\) can be defined on the space \\(V\\) as \\[\\ve{u}_v = \\m{V}^{-1}\\m{W}\\ve{u}_w.\\] It can be seen that in both cases, the original vector is first transformed to the space vector with standard basis (left-multiplying the basis matrix) and then transformed to the desired vector space (left-multiplying the basis matrix inverse ). 6.1.3 Change of Basis for Linear Transformations Previously, we have presented a linear transformation \\(f(\\ve{x})=\\m{A}\\ve{x}:\\real^n\\rightarrow\\real^m\\) using standard basis. This transformation can also be represented from a vector space \\(V\\) with basis \\(\\{\\ve{v}_1,\\dots,\\ve{v}_n\\}\\) to a vector space \\(W\\) with basis \\(\\{\\ve{w}_1,\\dots,\\ve{w}_n\\}\\), then \\(f&#39;: V \\rightarrow W\\) is defined as \\[f&#39;(\\ve{x}_v) = \\m{W}^{-1}\\m{A}\\m{V}\\ve{x}_v,\\] where the matrices \\(\\m{W}\\) and \\(\\m{V}\\) are the basis matrix of the vector spaces \\(W\\) and \\(V\\) respectively. The matrix multiplication \\(\\m{W}^{-1}\\m{A}\\m{V}\\) implies a change of basis from to standard basis, the linear transformation using the standard basis, and the change from the standard basis to the space \\(W\\). In cases that \\(V=W\\), then the linear transformation is defined as \\[f&#39;(\\ve{x}_v) = \\m{V}^{-1}\\m{A}\\m{V}\\ve{x}_v.\\] 6.1.4 Eigenvalues and Eigenvectors Eigenvalues and eigenvectors are used in several concepts of statistical inference and modelling. It can be useful for dimension reduction, decomposition of variance-covariance matrices, so on. For this reason, we provide basic details about eigenvectors and eigenvalues and their close relationship with linear transformations. 6.1.4.1 Definition The eigenvector of a linear transformation \\(\\m{A}_{n\\times n}\\) is a non-zero vector \\(\\ve{v}\\) such as the linear transformation of this vector is proportional to itself: \\[\\m{A}\\ve{v} = \\lambda \\ve{v} \\iff (\\m{A}-\\lambda\\m{I})\\ve{v} = \\ve{0},\\] where \\(\\lambda\\) is the eigenvalue associated to the eigenvector \\(\\ve{v}\\). The equation above has non-zero solution if and only if \\[\\det(\\m{A}-\\lambda\\m{I}) = 0.\\] Then, all the eigenvalues \\(\\lambda\\) of \\(\\m{A}\\) hold the condition above. There is an equivalence between the linear transformation \\(f(\\ve{x}) = \\m{A}\\ve{x}\\), and the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\) and eigenvectors \\(\\ve{v}_1, \\ve{v}_2, \\dots, \\ve{v}_n\\) of itself. This relationship provide more useful interpretation of the eigenvalues and eigenvectors, we will use the change of basis concept to describe it. 6.1.4.2 Eigendecomposition and geometric interpretation Considering a vector space \\(V\\) with basis \\(\\{\\ve{v}_1, \\ve{v}_2, \\dots, \\ve{v}_n\\}\\), any vector \\(\\ve{x} \\in \\real^n\\) can be represented as \\(\\m{V}\\ve{x}_v\\), where \\(\\ve{x}_v\\) is the representation of \\(\\ve{x}\\) using the matrix of basis \\(\\m{V}=(\\ve{v}_1, \\dots, \\ve{v}_n)\\) of the vector space \\(V\\). Then, the linear transformation can be expressed as \\[f(\\ve{x}) = \\m{A}\\ve{x} = \\m{A}\\m{V}\\ve{x}_v = \\m{V}\\m{D}\\ve{x}_v,\\] where the diagonal matrix \\(D=\\diag{\\lambda_1, \\dots, \\lambda_n}\\) and the last equivalence hold because \\(\\m{A}\\ve{v}_i=\\ve{v}_i\\lambda_i\\). Finally, expressing \\(\\ve{x}_v\\) in terms of the vector \\(\\ve{x}\\) defined on the standard basis, we obtain that \\[f(\\ve{x}) = \\m{V}\\m{D}\\m{V}^{-1}\\ve{x},\\] the equality \\(\\m{A}=\\m{V}\\m{D}\\m{V}^{-1}\\) is called eigendecomposition. Hence, the linear transformation is equivalent to the following: change the basis of \\(\\ve{x}\\) to the vector space \\(V\\) , apply the diagonal linear transformation \\(D\\) and return to the space with standard basis. Geometrically, you can think of \\(\\{\\ve{v}_1, \\ve{v}_2, \\dots, \\ve{v}_n\\}\\) as the basis of vectorial space \\(V\\) where the transformation \\(\\m{A}\\) becomes only an scaling transformation \\(\\m{D}\\) and the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\) are the scaling factor in direction of the corresponding eigenvector \\(\\ve{v}_1, \\ve{v}_2, \\dots, \\ve{v}_n\\). 6.1.4.3 Basis properties There are certain properties the are useful for statistical modelling such as: Trace of \\(\\m{A}\\) is equals to the sum of the eigenvalues. Determinant of \\(\\m{A}\\) is equals to the sum of the eigenvalues. If \\(\\m{A}\\) is symmetric, then all eigenvalues are real. If \\(\\m{A}\\) is positive definite, then all eigenvalues are positive. Note that, some of these properties can be explained using the eigendecomposition \\(\\m{A} = \\m{V}\\m{D}\\m{V}^{-1}\\). 6.1.5 Cauchy–Schwartz inequality \\(|&lt;u, v&gt;| &lt;= ||u|| * ||v||\\) "],
["710-optimization.html", "Chapter 7 here Chapter 8 Newton-Raphson and Gradient Descent", " Chapter 7 here Computational Aspects: The condition number of an square matrix is the ratio between the largest and the smallest singular value. For a symmetric positive-definite matrix this is the same as the ratio between the largest and smallest eigenvalue. The importance of this indicator is that it measure how numerically unstable are matrix operations, e.g. inversion. Chapter 8 Newton-Raphson and Gradient Descent Given a function \\(f(x)\\), the minimum can be found using gradient descent which needs to compute the first derivative \\(f&#39;(x)\\), or using newton-rapson to find the solution of \\(f&#39;(x) = 0\\) requiring to compute \\(f&#39;&#39;(x)\\). Gradient descent is mainly used because it only need the first derivative \\(f&#39;(x)\\). "],
["910-references.html", "Chapter 9 Here we go References", " Chapter 9 Here we go References "]
]
